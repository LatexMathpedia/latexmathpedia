---
title: "Modelos para Vectores Aleatorios"
date: "2025-9-22"
description: "Apuntes sobre modelos para vectores aleatorios discretos y continuos. Distribución multinomial y distribución normal multivariante."
tags: ["Matemáticas", "Geometría", "Teoremas"]
---

## Introducción
Al igual que en el caso unidimensional, en el caso multidimensional también se pueden definir variables aleatorias. En este caso, se trata de vectores aleatorios. En este tema se estudiarán los modelos más importantes para vectores aleatorios discretos y continuos que permitan describir la variabilidad conjunta de las variables aleatorias que los componen. Estos prototipos se eligen de modo que:
  - Coincidan o se ajusten bin a las distribuciones reales
  - Que sean manejables desde el punto de vista matemático

## Modelos para vectores aleatorios discretos
  ### Distribución multinomial
  Supongamos que se considera un experimento aleatorio consistente en repetir cierto número $n$ de veces y de forma independiente un experimento cuyo resultado esté asociado a la ocurrencia de uno y sólo uno de $k$ posibles sucesos $A_1, \ldots, A_k$.

  Asociada al experimento aleatorio global se define el vector aleatorio $(X_1, \ldots, X_k)$ con **distribución multinomial de parámetros $n$ y $(p_1, \ldots, p_k)$** como aquel que a cada resultado del experimento aleatorio asocial el valor vectorial:
  $$
    \left(\begin{array}{c}
      \text{\small nº de veces que ocurre} A_1 \text{\small en las } n \, \text{\small realizaciones del experimento }, \ldots, \\
      , \ldots, \text{\small nº de veces que ocurre} A_k \text{\small en las } n \, \text{\small realizaciones del experimento}
    \end{array}\right)
  $$
  Es decir, $(X_1, \ldots, X_k)$ es un vector $k$ dimensional donde $X_i$ es "el número de veces que ocurre $A_i$ de entre $n$ realizaciones experimentales independientes de un experimento" entonces:
  $$
    P(X_1 = x_1, \ldots, X_k = x_k) = P\left(\begin{array}{c}
     \text{\small en } n \, \text{\small realizaciones independientes del experimento} \\
     \text{\small aparece } x_1 \, \text{\small veces } A_1, \ldots, x_k \, \text{\small veces } A_k 
    \end{array}\right)
  $$
  Y su probabilidad viene dada por:
  $$
    P(X_1 = x_1, \ldots, X_k = x_k) = \dfrac{n!}{x_1! \ldots x_k!} [P(A_1)]^{x_1} \cdot \ldots \cdot [P(A_k)]^{x_k} = \dfrac{n!}{x_1! \ldots x_k!} p_1^{x_1} \ldots p_k^{x_k}
  $$  
  O, si usamos notación de combinaciones:
  $$
    P(X_1 = x_1, \ldots, X_k = x_k) = \binom{n}{x_1, \ldots, x_k} p_1^{x_1} \ldots p_k^{x_k} = PR_{n, x_1, \ldots, x_k} p_1^{x_1} \ldots p_k^{x_k}\\
  $$
  *NOTACIÓN*: $\,(X_1, \dots, X_k) \rightsquigarrow M(n, p_1, \dots, p_k)$
  
  Notar que: $n \in \mathbb{N}$, $p_1, \dots, p_k \in (0, 1)$ y $p_1 + \dots + p_k = 1$

  > La multinomial es una generalización de la binomial (que corresponde con el caso $k = 2$). En este caso nos interesa el estudio de la ocurrencia de varios sucesos y no sólo la de uno.
  
  ##### Ejemplo
    Podemos llegar de forma ``intuitiva'' a la fórmula de la distribución multinomial, para ello, consideremos el experimento aleatorio en el que extraemos $n$ bolas de forma independiente (con reposición) de una urna con bolas de colores. Así, definimos:
    $$
      X_i = \text{nº de bolas extraídas de tipo } i \quad \text{ con } i = 1, \ldots, k
    $$
    Así, definimos el vector aleatorio $(X_1, \ldots, X_k)$ vector $k$-dimensional.

    Ahora queremos estudiar el comportamiento o distribución de $(X_1, \ldots, X_k)$, es decir, queremos conocer:
    $$
      P\left((X_1 = x_1) \cap \ldots \cap (X_k = x_k)\right) \quad \text{ con } x_1 + \ldots + x_k = n
    $$
    Basta notar que, como las bolas se extraen de forma independiente, y por ello, consideramos las probabilidades de que salga una bola de tipo $i$ como:
    $$
      P(X_i = x_i) = p_i^{x_i}
    $$
    Y como la probabilidad de que salga una bola de tipo $i$ es independiente de la probabilidad de que salga una bola de tipo $j$, tenemos que:
    $$
      P(X_1 = x_1 \cap \ldots \cap X_k = x_k) = \binom{n}{x_1} \binom{n - x_1}{x_2} \ldots \binom{n - x_1 - \ldots - x_{k - 1}}{x_k} p_1^{x_1} p_2^{x_2} \ldots p_k^{x_k}
    $$
    Basta desarrollar el producto de los coeficientes binomiales:
    $$
      \binom{n}{x_1} \binom{n - x_1}{x_2} \binom{n - x_1 - x_2}{x_3} \dots = \frac{n!}{x_1!\cancel{(n - x_1)!}} \frac{\cancel{(n - x_1)!}}{x_2!\cancel{(n - x_1 - x_2)!}} \ldots = \frac{n!}{x_1! \ldots x_k!}
    $$
    Y así llegamos a que:
    $$
      P(X_1 = x_1 \cap \ldots \cap X_k = x_k) = \frac{n!}{x_1! \ldots x_k!} p_1^{x_1} \ldots p_k^{x_k}
    $$
   

  #### Probabilidad marginal en una multinomial
  Puede resultar interesante conocer la probabilidad marginal de una variable aleatoria en una multinomial. Así, si queremos hallar la probabilidad de que $X_i = x$, y esta vendría dada por:
  $$
    \sum_{\substack{x_1 + \ldots + x_k = n\\x_1 + \dots + x_{i - 1} + x_{i + 1} \dots + x_k = n - x }} P(X_1 = x_1; \ldots; X_{i - 1} = x_{i - 1}; X_i = x; X_{i + 1} = x_{i + 1}, \ldots; X_k = x_k) 
  $$
  Y aplicando la fórmula de la probabilidad, llegamos a que la marginal es:
  $$ 
    \begin{align*}
      & \sum_{x_1, \ldots, x_{i - 1}, x_{i + 1}, \ldots, x_k} \dfrac{n!}{x_1! \ldots x_{i - 1}! x! x_{i + 1}! \ldots x_k!} p_1^{x_1} \ldots p_{i - 1}^{x_{i - 1}} p_i^x p_{i + 1}^{x_{i + 1}} \ldots p_k^{x_k} = \\[4ex]
    & \hspace{4ex} = \dfrac{n!}{x!} p_i^x \sum_{x_1, \ldots, x_{i - 1}, x_{i + 1}, \ldots, x_k} \dfrac{1}{x_1! \ldots x_{i - 1}! x_{i + 1}! \ldots x_k!} p_1^{x_1} \ldots p_{i - 1}^{x_{i - 1}} p_{i + 1}^{x_{i + 1}} \ldots p_k^{x_k} = \\[4ex]
    & \hspace{4ex} = \dfrac{n!}{x!(n - x)!} \sum_{\substack{x_1, \ldots, x_{i - 1}, x_{i + 1}, \ldots, x_k\\x_1 + \ldots + x_{i - 1} + x_{i + 1} + \ldots + x_k = n - x}} \dfrac{(n - x)!}{x_1! \ldots x_{i - 1}! x_{i + 1}! \ldots x_k!} p_1^{x_1} \ldots p_{i - 1}^{x_{i - 1}} p_{i + 1}^{x_{i + 1}} \ldots p_k^{x_k}
    \end{align*}
    $$
    Aplicando la fórmula de Leibniz para el desarrollo de un binomio:
    $$
      \sum_{x_1, \ldots, x_k = n} \dfrac{n!}{x_1 \dots x_k!} a_1^{x_1} \ldots a_k^{x_k} = (a_1 + \ldots + a_k)^n
    $$
    Podemos llegar a que la marginal anterior se puede expresar como:
    $$
    P(X_i = x) =  \dfrac{n!}{x!(n - x)!} p_i^x (p_1 + \ldots + p_{i - 1} + p_{i + 1} + \ldots + p_k)^{n - x} = \dfrac{n!}{x!(n - x)!} p_i^x (1 - p_i)^{n - x}
    $$
    Así, hemos llegado a que la marginal de una variable aleatoria en una multinomial es una distribución binomial de parámetros $n$ y $p_i$, es decir:
    $$
      X_i \rightsquigarrow B(n, p_i)\\
    $$

    #### Esperanza y varianza en las marginales de una multinomial
    Dado que la marginal de una variable aleatoria en una multinomial es una binomial, podemos hallar sus esperanzas y varianzas de forma sencilla:
    $$
      E(X_i) = n \cdot p_i \quad \text{ y } \quad Var(X_i) = n \cdot p_i \cdot (1 - p_i)\\
    $$

    #### Esperanza y varianza de una multinomial de orden 2
    En el caso de la multinomial de orden dos, podemos hallar la esperanza a través de:
    $$
      E(X_i X_j) = \sum_{x_i, x_j} x_i \cdot x_j P(X_i = x_i; X_j = x_j)
    $$


  ##### Desmotración. Cáluclo de la marginal de orden 2 de una multinomial
    Sea $(X_1,\ldots, X_k) \rightsquigarrow \mathcal{M}(n, p_1, \ldots, p_k)$, queremos hallar la marginal del vector bidimensional $(X_1, X_2)$. Así, como $(X_1, X_2)$ es una variable aleatoria bidimensional discreta, entonces, supongamos que queremos hallar la probabilidad de que $X_1 = x_1$ y $X_2 = x_2$, es decir:
    $$
      P(X_1 = x_1; X_2 = x_2) = \sum_{\substack{x_3, \ldots, x_k\\x_3 +  \ldots +  x_k = n - x_1 - x_2}} P(X_1 = x_1; X_2 = x_2; X_3 = x_3; \ldots; X_k = x_k)
    $$
    Y esto es equivalente a:
    $$
      \begin{align*}
      P(X_1 = x_1; X_2 = x_2) & = \sum_{x_3, \ldots, x_k} \dfrac{n!}{x_1! x_2! x_3! \ldots x_k!} p_1^{x_1} p_2^{x_2} p_3^{x_3} \ldots p_k^{x_k}	 = \\[2ex]
      & = \frac{n!}{x_1!x_2!} p_1^{x_1}p_2^{x_2} \sum_{x_3, \ldots, x_k} \dfrac{1}{x_3! \ldots x_k!} p_3^{x_3} \ldots p_k^{x_k} = \\[3ex]
      & = \frac{n!}{x_1!x_2!(n - x_1 - x_2)!} p_1^{x_1}p_2^{x_2} \sum_{x_3, \ldots, x_k} \dfrac{(n - x_1 - x_2)!}{x_3! \ldots x_k!} p_3^{x_3} \ldots p_k^{x_k} = \\[3ex]
      & = \frac{n!}{x_1!x_2!(n - x_1 - x_2)!} p_1^{x_1}p_2^{x_2} (p_3 + \ldots + p_k)^{n - x_1 - x_2} = \\[3ex]
      & = \frac{n!}{x_1!x_2!(n - x_1 - x_2)!} p_1^{x_1}p_2^{x_2} (1 - p_1 - p_2)^{n - x_1 - x_2}
      \end{align*}
    $$
    Y, de hecho, lo que hemos obtenido es una distribución multinomial de parámetros:
    $$
      (X_1, X_2) \rightsquigarrow \mathcal{M}(n, p_1, p_2, 1 - p_1 - p_2)
    $$

  
  #### Covarianza de una marginal de orden 2 de una multinomial
  También, podemos comprobar las covarianzas:
  $$
    \text{Cov}(X_i, X_j) = E(X_i \cdot X_j) - E(X_i)E(X_j)
  $$
  Donde esta esperanza se puede calcular como:
  $$
    E(X_iX_j) = \sum_{x_ix_j} x_i \cdot x_j P(X_i = x_i; X_j = x_j)
  $$
  Y así, llegamos a que:
  $$
    \text{Cov}(X_i, X_j) = -n \cdot p_i \cdot p_j
  $$

  ##### Demostración
    Sea $(X_1, \ldots X_k) \rightsquigarrow \mathcal{M}(n, p_1, \ldots, p_k)$, queremos hallar la covarianza de $X_i$ y $X_j$. Así, como $(X_1, X_2)$ es una variable aleatoria bidimensional discreta, entonces, supongamos que queremos hallar la covarianza de $X_1$ y $X_2$, es decir:
    $$
      \text{Cov}(X_1, X_2) = E(X_1X_2) - E(X_1)E(X_2)
    $$
    Como hemos visto previamente:
    $$
      P(X_1 = x_1, X_2 = x_2) = \dfrac{n!}{x_1!x_2!(n - x_1 - x_2)!} p_1^{x_1}p_2^{x_2} (1 - p_1 - p_2)^{n - x_1 - x_2}
    $$
    Y sabemos que la esperanza conjunta de $X_1$ y $X_2$ viene dada por:
    $$
      E(X_1X_2) = \sum_{x_1 = 0}^{n} \sum_{x_2 = 0}^{n - x_1} x_1 x_2 P(X_1 = x_1, X_2 = x_2)
    $$
    Así, desarrollando obtenemos que:
    $$
      \begin{align*}
      E(X_1 X_2) & = \sum_{x_1 = 0}^{n} \sum_{x_2 = 0}^{n - x_1} x_1 x_2 \dfrac{n!}{x_1!x_2!(n - x_1 - x_2)!} p_1^{x_1}p_2^{x_2} (1 - p_1 - p_2)^{n - x_1 - x_2} =\\[4ex]
      & = n! \sum_{x_1 = 0}^{n} \dfrac{p_1^{x_1}}{x_1!} x_1 \sum_{x_2 = 0}^{n - x_1} \dfrac{p_2^{x_2}}{x_2!} x_2 \dfrac{(1 - p_1 - p_2)^{n - x_1 - x_2}}{(n - x_1 - x_2)!} =\dots
      \end{align*}
    $$
    Y a partir de aquí podemos desarrollar la suma y llegar al resultado (espero)
  

  #### Matriz de varianzas y covarianzas de una multinomial
  Como consecuencia de esto, podemos hallar la matriz de varianzas y covarianzas de una multinomial. Así, si consideramos la matriz de varianzas y covarianzas de un vector aleatorio $(X_1, \ldots, X_k)$ con distribución multinomial de parámetros $n$ y $(p_1, \ldots, p_k)$, entonces, la matriz de varianzas y covarianzas viene dada por:
  $$
    \Sigma_{(X_1, \ldots, X_k)} = \begin{pmatrix} np_1 (1 - p_1) & \cdots & - np_1p_k\\
    \vdots & \ddots & \vdots\\
    -np_kp_1 & \cdots & np_k(1 - p_k)\end{pmatrix}
  $$
  Análogamente, el vector de esperanzas vendría dado por:
  $$
    E(X_1, \ldots, X_k) = \begin{pmatrix} np_1& \vdots& np_k\end{pmatrix}
  $$


  ## Modelos para vectores aleatorios continuos
  ### Distribución uniforme en un recinto continuo y acotado $B \in \mathcal{B}_{\mathbb{R}^k}$
  Se dice que un vector aleatorio $X = (X_1, \ldots, X_k)$ tiene **distribución uniforme en el recinto $B \in \mathcal{B}_{\mathbb{R}^k}$** si su función de densidad conjunta viene dada por:
  $$
    f(x_1, \ldots, x_k) = \left\{
      \begin{array}{ll}
        \dfrac{1}{\lambda(B)} & \text{ si } (x_1, \ldots, x_k) \in B\\
        0 & \text{ en otro caso}
      \end{array}
    \right.
  $$
  donde $\lambda(B)$ es la medida de Lebesgue en $B$. 

  *NOTACIÓN*: $X \rightsquigarrow \mathcal{U}(B)$

  ### Distribución normal $k$-dimensional
  Es un modelo asociado a vectores que (idealmente) toman cualquier valor vectorial de acuerdo con una distribución simétrica y campaniforme en todas sus direcciones (más verosímiles los valores cuanto más centrales y menos cuanto más extremos). Se dice que $X$ tiene **distribución normal de parámetros $\mu$ y $k$** si su función de densidad conjunta viene dada por:
  $$
    f(x_1, \ldots, x_k) = \dfrac{1}{\sqrt{(2\pi)^k \cdot |\Sigma|}} e^{ -\frac{1}{2}(x - \mu) \Sigma^{ - 1} (x - \mu)^t}
  $$
  donde $\mu = \left(E(X_1), \ldots, E(X_k)\right)$ es el vector de medias y $\Sigma = \Sigma_{(X_1, \ldots, X_k)}$ la matriz de varianzas y covarianzas de $X$, es decir:
  $$
    \Sigma = \begin{pmatrix} \sigma^2_1 & \cdots & \text{Cov}(X_1, X_k)\\
    \vdots & \ddots & \vdots\\
    Cov(X_k, X_1) & \cdots & \sigma_k^2\end{pmatrix} 
  $$
  *NOTACIÓN*: $X \rightsquigarrow \mathcal{N}(\mu, \Sigma)$

  ##### Nota
    En particular, para el caso de $k = 2$, se puede desarrollar su función de densidad conjunta como:
    $$
      f(x, y) = \dfrac{1}{2\pi\sigma_X\sigma_Y \sqrt{1 - \rho_{XY}^2}} e^{ - \frac{1}{2(1 - \rho_{XY}^2)} \left[\left(\frac{x - \mu_X}{\sigma_X}\right)^2 - 2\rho_{XY}\left(\frac{x - \mu_X}{\sigma_X}\right)\left(\frac{y - \mu_Y}{\sigma_Y}\right) + \left(\frac{y - \mu_Y}{\sigma_Y}\right)^2\right]}
    $$
    Y se denota como $\mathcal{N}(\mu_X, \mu_Y, \sigma_X, \sigma_Y, \rho_{XY}) = \mathcal{N}(E(X), E(Y), \sigma_X, \sigma_Y, \rho_{XY})$ con:
    $$
      \sigma_X = \sqrt{\text{Var}(X)} \quad \text{y} \quad \rho_{XY} = \dfrac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y}
    $$
  
  #### Propiedades de la distribución normal $k$-dimensional
  Puede verificarse que si $(X_1, \ldots, X_k) \rightsquigarrow \mathcal{N}(\mu, \Sigma)$ donde:
  $$
    \mu = \begin{pmatrix} \mu_1 & \cdots & \mu_k \end{pmatrix} \in \mathcal{R}^k \quad \text{y} \quad \Sigma = \left[\sigma_{i, j}\right]_{i, j = 1, \ldots, k} \in \mathcal{M}_{k \times k}(\mathbb{R}) 
  $$
  donde $\Sigma$ es una matriz simétrica definida positiva entonces:
  - Las variables unidimensionales componentes (marginales) son:
    $$
      X_i \rightsquigarrow \mathcal{N}(\mu_i, \sigma_i^2) \quad \text{con} \quad i = 1, \ldots, k
    $$

  >  Notar que el contrario no es cierto, es decir, que si las componentes son normales, no implica que el vector sea normal.
    
  - $\text{Cov}(X_i, X_j) = \sigma_{i,j} $ para todo $i, j = 1, \ldots, k$
  - Las variables unidimensionales componentes $X_i$ son 2 a 2 independientes si y solo si $\Sigma$ es diagonal. Además, en este caso se tiene que:
    $$
      \text{Var}(X_1 + \ldots + X_k) = \text{Var}(X_1) + \ldots + \text{Var}(X_k)
    $$